{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ml3.png' width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Process\n",
    "\n",
    "### The machine learning process is much more involved than the high level work-flow depicted in the previous cell. \n",
    "\n",
    "- 1. The stages can be broadly defined as follows:\n",
    "    - Data exploration\n",
    "    - Understand the feature data (data types, missing values, outliers, etc)\n",
    "    - Visualization (boxplots, scatter plots, correlations matrix, etc)  \n",
    "    - Correlations between features and the target\n",
    "    - Feature engineering (aggregate features)\n",
    "- 2. Data preparation\n",
    "    - Dealing with outliers \n",
    "    - Dealing with missing values\n",
    "    - Feature encoding (encoding categorical features)\n",
    "    - Feature selection\n",
    "    - Feature scaling\n",
    "    - Handling Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ml4.png' width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The machine learning process__\n",
    "\n",
    "### The stages can be broadly defined as follows:\n",
    "\n",
    "- Building and Evaluating Models\n",
    "    - Train many models from different categories (e.g., linear, kNN, etc.) using standard parameters.\n",
    "    - Measure and compare their performance.\n",
    "    - Debug ML models and analyse the types of errors the models make.\n",
    "\n",
    "- Fine Tuning and Optimization\n",
    "    - Perform hyper-parameter optimization (e.g. tuning value of k in kNN) \n",
    "    - Finally assess the generalization capability of your model on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of Today's topics\n",
    "\n",
    "- Today, I will briefly introduce Scikit learn and how to use it to perform basic classification and regression modelling\n",
    "\n",
    "    - Therefore, the structure of what we cover will be:\n",
    "    - Introduction to Scikit Learn\n",
    "    - Data Preparation (Train and Test data)\n",
    "    - Build Classification Model\n",
    "    - Building Regression Model\n",
    "    - Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-learn provides a range of supervised and unsupervised learning algorithms in Python.\n",
    "\n",
    "- The library is built upon the following:\n",
    "    - __NumPy__: Base n-dimensional array package\n",
    "    - __SciPy__: Fundamental library for scientific computing\n",
    "    - __Matplotlib__: Comprehensive 2D/3D plotting\n",
    "\n",
    "- The library is focused on modelling data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn is well organized and there is a wealth of tutorial and API pages, which can be accessed here. \n",
    "\n",
    "- The functionally offered by Scikit-learn can be broken into the following :\n",
    "\n",
    "    - __Classification__: a large collection of learning algorithms such as naive bayes, support vector machines, decision trees, ensembles etc.\n",
    "    - __Clustering__: for grouping unlabelled data such as Kmeans, DBScan, etc.\n",
    "    - __Regression__: libraries for predicting real-valued attributes such as multiple linear regression, ridge regression, etc. \n",
    "    - __Pre-processing__: Outlier detection, normalization, encoding categorical features.\n",
    "    - __Dimensionality Reduction__: Reduces the number of features that you need to consider in your dataset.\n",
    "    - __Model Selection__: Comparing, validating and choosing parameters and models, metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ml5.png' width=900/>\n",
    "<img src='ml6.png' width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few important notes about Scikit Learn\n",
    "\n",
    "- The following are some important requirements that you should keep in mind when working with Scikit learn. \n",
    "\n",
    "    - Features and classes/target values are separate objects (data structures)\n",
    "    - Features and classes should be numerical\n",
    "    - Features and classes should be NumPy arrays\n",
    "    - Features and classes should have a specific shape\n",
    "    - Features should be 2D (Columns correspond to numbers of features and rows are number of data instances)\n",
    "    - Class array should be one dimensional with same number of instances as there are data instances in the features array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-learn comes with a number of standard example datasets. These are broken into [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) and [real-world datasets](https://scikit-learn.org/stable/datasets/real_world.html). \n",
    "    - Toy datasets include the __iris__ dataset and __digits__ datasets for classification and the __Boston house prices__ dataset for regression. \n",
    "    - Real-world datasets include Olivetti faces dataset, newsgroups, California housing dataset, etc. \n",
    "\n",
    "- These datasets are dictionary-like objects holding at least two items: \n",
    "\n",
    "    - A NumPy array of shape n_samples * n_features with the key data \n",
    "    - A NumPy array of length n_samples, containing the class values, with key target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "iris = datasets.load_iris() # Load iris dataset into a dataset object\n",
    "\n",
    "print (iris.data.shape) # Outputs the dimensions of the data in this case (150, 4)\n",
    "\n",
    "print (iris.target_names) # Name of three calsses\n",
    "\n",
    "print (iris.feature_names) # Name of four feats\n",
    "\n",
    "#print (iris.DESCR) # description\n",
    "\n",
    "print (iris.data) #Accesses the data stored in the dataset object (2D numpy array)\n",
    "\n",
    "#print (iris.data[0:1, 2:4]) # Accesses first row of the dataset but just columns with index 2 and 3\n",
    "\n",
    "print (iris.target) # Accesses the class associated with each data item\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "\n",
    "\n",
    "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
    "                [4.9, 3. , 1.4, 0.2],\n",
    "                [4.7, 3.2, 1.3, 0.2],\n",
    "                [4.6, 3.1, 1.5, 0.2],...\n",
    "'target': array([0, 0, 0, ... 1, 1, 1, ... 2, 2, 2, ...\n",
    "'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), \n",
    "...}\n",
    "    \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ml7.png' width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine dataset\n",
    "\n",
    "<img src='ml8.png' width=900/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 14)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14.23,  1.71,  2.43, ...,  5.64,  1.04,  3.92],\n",
       "       [13.2 ,  1.78,  2.14, ...,  4.38,  1.05,  3.4 ],\n",
       "       [13.16,  2.36,  2.67, ...,  5.68,  1.03,  3.17],\n",
       "       ...,\n",
       "       [13.27,  4.28,  2.26, ..., 10.2 ,  0.59,  1.56],\n",
       "       [13.17,  2.59,  2.37, ...,  9.3 ,  0.6 ,  1.62],\n",
       "       [14.13,  4.1 ,  2.74, ...,  9.2 ,  0.61,  1.6 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = np.genfromtxt(\"wine.data\", delimiter=\",\")\n",
    "\n",
    "print (df.shape)\n",
    "\n",
    "target = df[:, 0]\n",
    "\n",
    "data = df[:, 1:13]\n",
    "print (target)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ml.png' width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test data\n",
    "\n",
    "<img src='ml2.png' width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Using a Logistic Regression  Classifier in SciKit Learn\n",
    "\n",
    "\n",
    "- Logistic Regression are a family of supervised learning methods (estimators). \n",
    "\n",
    "- The class we will be using is sklearn.linear_model.LogisticRegression\n",
    "\n",
    "- The LogisticRegression takes as input two arrays: \n",
    "    - An array X of size [n_samples, n_features] holding the training samples, \n",
    "    - An array Y of integer values, size [n_samples], holding the class labels for the training samples. \n",
    "\n",
    "- The __LogisticRegression__, as you can see from the API, takes many more arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train the classifier pass it the training data and classes\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# predict the class for an unseen example\n",
    "prediction = clf.predict( [[4.9, 3.0,  1.4, 0.2]])\n",
    "\n",
    "#corresponds to setosa\n",
    "\n",
    "print (prediction)\n",
    "\n",
    "\n",
    "prediction = clf.predict( [[4.5, 0.9, 13.0, 2.1]])\n",
    "\n",
    "print (prediction)\n",
    "# which corresponds to virginica\n",
    "\n",
    "# When we run this code it will predict the output for this unseen instance as being 2,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using kNN on Iris Dataset\n",
    "\n",
    "- In the next example we will apply k-nearest neighbour to the iris dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import neighbors\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 8)  \n",
    "# We can provide many different parameters to machine learning algorithms in Scikit Learn.\n",
    "#However, most have default values allows us to get started very quickly.\n",
    "\n",
    "knn.fit(iris.data, iris.target)\n",
    "\n",
    "print (knn.predict([[3, 5, 4, 2]]))\n",
    "print (knn.predict([[0.3, 5.0, 4.0, 0.2]]))\n",
    "\n",
    "print (knn.predict([[0.3, 4.1, 4.0, 0.2]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can provide many different parameters to machine learning algorithms in Scikit Learn. However, most have default values allows us to get started very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import neighbors\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5, algorithm = \"kd_tree\")\n",
    "\n",
    "knn.fit(iris.data, iris.target)\n",
    "\n",
    "print (knn.predict([[3, 5, 4, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below we look at two separate scenarios for simple evaluation (we will cover/ have covered more advanced and realistic techniques in a later):\n",
    "\n",
    "- There is a separate __training__ (left) and __test__ (right) dataset that can be used. \n",
    "- A __single__ dataset split into a training and test data (holdout method). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a separate training and test dataset that can be used\n",
    "\n",
    "<img src='ml9.png' width=900/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assesing Accuracy\n",
    "\n",
    "- Assuming I have separate training and test data I might have the following arrays\n",
    "    - features_train, labels_train\n",
    "    - features_test, labels_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-298665690eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# trains the classifier pass it the training data and classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# predict the class for an unseen example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "# creates a new decision tree object classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# trains the classifier; pass it the training data and classes\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "\n",
    "# predict the class for an unseen example\n",
    "results= clf.predict(features_test)\n",
    "\n",
    "print (metrics.accuracy_score(results, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice in this example when we call predict we are passing it as __2D NumPy__ array. \n",
    "- The accuracy_score function (available in the metrics module) will count the number of classes we correctly predicated and express that as a percentage of the total number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test/train split\n",
    "\n",
    "<img src='ml10.png' width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Accuracy (Splitting Training Data)\n",
    "\n",
    "- In scikit-learn a random split into training and test sets can be quickly computed with the train_test_split helper function. \n",
    "- As arguments we pass it the original data and target as well as the percentage of the original data we want for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178,)\n",
      "(142, 13) (142,)\n",
      "(36, 13) (36,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = model_selection.train_test_split( wine.data, wine.target, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "print (wine.data.shape, wine.target.shape)\n",
    "print (train_features.shape, train_labels.shape)\n",
    "\n",
    "print (test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9722222222222222\n",
      "0.9722222222222222\n",
      "0.75\n",
      "0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "train_features, test_features, train_labels, test_labels = model_selection.train_test_split( wine.data, wine.target, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "# trains the classifier pass it the training data and classes\n",
    "clf = clf.fit(train_features, train_labels)\n",
    "\n",
    "\n",
    "# predict the class for an unseen example\n",
    "results= clf.predict(test_features)\n",
    "#print (results)\n",
    "print  (metrics.accuracy_score(results, test_labels))\n",
    "\n",
    "\n",
    "lrclf = LogisticRegression(solver='liblinear')\n",
    "\n",
    "lrclf = lrclf.fit(train_features, train_labels)\n",
    "\n",
    "# predict the class for an unseen example\n",
    "results= clf.predict(test_features)\n",
    "#print (results)\n",
    "print  (metrics.accuracy_score(results, test_labels))\n",
    "\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 2)\n",
    "# predict the class for an unseen example\n",
    "knn = knn.fit(train_features, train_labels)\n",
    "\n",
    "results= knn.predict(test_features)\n",
    "#print (results)\n",
    "print  (metrics.accuracy_score(results, test_labels))\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "# predict the class for an unseen example\n",
    "knn = knn.fit(train_features, train_labels)\n",
    "\n",
    "results= knn.predict(test_features)\n",
    "#print (results)\n",
    "print  (metrics.accuracy_score(results, test_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We take in the data. We split into a training set and a test set. We then assess its accuracy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99946921e-01 3.96829653e-05 1.33961632e-05]\n",
      " [2.18055785e-09 1.24830618e-09 9.99999997e-01]\n",
      " [1.88590347e-05 9.99981130e-01 1.14318547e-08]\n",
      " [9.99537224e-01 4.49282186e-04 1.34934725e-05]\n",
      " [2.51007147e-04 9.99748746e-01 2.46384206e-07]\n",
      " [1.77526777e-03 9.98220913e-01 3.81924159e-06]\n",
      " [9.99999592e-01 1.63946274e-07 2.43776141e-07]\n",
      " [1.55913142e-06 5.05752038e-07 9.99997935e-01]\n",
      " [1.00566401e-03 9.98993600e-01 7.35948458e-07]\n",
      " [5.24727421e-05 9.99946548e-01 9.78953094e-07]\n",
      " [1.39912898e-03 4.21297924e-04 9.98179573e-01]\n",
      " [1.25219841e-06 1.38023156e-04 9.99860725e-01]\n",
      " [9.99999976e-01 9.60247566e-09 1.44318761e-08]\n",
      " [5.17674155e-02 9.48232569e-01 1.56729812e-08]\n",
      " [1.64312701e-07 1.01472481e-07 9.99999734e-01]\n",
      " [4.32540461e-07 9.99999560e-01 7.15263502e-09]\n",
      " [9.99977663e-01 8.91570063e-06 1.34208320e-05]\n",
      " [9.99999961e-01 1.95790916e-10 3.91953765e-08]\n",
      " [3.98939166e-05 4.75650032e-02 9.52395103e-01]\n",
      " [9.99992587e-01 7.41005719e-06 2.64742161e-09]\n",
      " [1.12953806e-03 9.98870447e-01 1.49097291e-08]\n",
      " [9.99488924e-01 5.08425775e-04 2.65033476e-06]\n",
      " [8.39324520e-01 1.60567906e-01 1.07573735e-04]\n",
      " [2.60156795e-03 9.97398358e-01 7.38533581e-08]\n",
      " [8.15114806e-03 8.34090546e-01 1.57758306e-01]\n",
      " [1.61397068e-06 9.99998358e-01 2.78901950e-08]\n",
      " [1.42427233e-05 9.99985704e-01 5.35076456e-08]\n",
      " [1.19082109e-06 9.99998614e-01 1.95401618e-07]\n",
      " [7.49592302e-01 2.38151682e-01 1.22560163e-02]\n",
      " [5.62992031e-09 1.31071350e-08 9.99999981e-01]\n",
      " [9.99997303e-01 2.69060006e-06 6.28858837e-09]\n",
      " [9.99534315e-01 4.52209719e-04 1.34752177e-05]\n",
      " [1.35799141e-01 8.64128906e-01 7.19529004e-05]\n",
      " [9.99996737e-01 1.09252396e-06 2.16998598e-06]\n",
      " [9.98564374e-01 1.43541177e-03 2.14453393e-07]\n",
      " [9.92744767e-01 7.25445281e-03 7.80048476e-07]]\n",
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "train_features, test_features, train_labels, test_labels = model_selection.train_test_split( wine.data, wine.target, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=3000).fit(train_features, train_labels)\n",
    "\n",
    "results = clf.predict_proba(test_features)\n",
    "\n",
    "print (results)\n",
    "\n",
    "pred = np.argmax(results, axis=1) # Returns the indices of the maximum values along an axis.\n",
    "\n",
    "print (metrics.accuracy_score(pred, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='reg1.png' width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])\n",
      "(404, 13)\n",
      "(102, 13)\n",
      "(404,)\n",
      "(102,)\n",
      "R-Squared: 0.7334492147453135\n",
      "MAE: 3.2132704958423437\n",
      "MSE: 20.869292183770348\n",
      "RMSE: 4.568292042303157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RHaque\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this case special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows:\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and:\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "print(boston.keys())\n",
    "\n",
    "\n",
    "#print(boston[\"data\"]) #prints the description of data\n",
    "#print(boston.data.shape)\n",
    "#print(boston.target.shape)\n",
    "#print(boston.feature_names)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(boston.data, boston.target, test_size = 0.2, random_state = 5)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "regression = linear_model.LinearRegression()\n",
    "regression.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "print(\"R-Squared:\", regression.score(X_test,Y_test))\n",
    "\n",
    "Y_pred = regression.predict(X_test)\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(Y_test, Y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(Y_test, Y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
